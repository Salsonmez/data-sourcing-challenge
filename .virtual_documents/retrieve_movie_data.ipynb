


# Dependencies
import requests
import time
from dotenv import load_dotenv
import os
import pandas as pd
import json
from pandas import json_normalize


# Set environment variables from the .env in the local environment
load_dotenv()

nyt_api_key = os.getenv("NYT_API_KEY")
tmdb_api_key = os.getenv("TMDB_API_KEY")





# Set the base URL
url = "https://api.nytimes.com/svc/search/v2/articlesearch.json?"

# Filter for movie reviews with "love" in the headline
# section_name should be "Movies"
# type_of_material should be "Review"
filter_query = 'section_name:"Movies" AND type_of_material:"Review" AND headline:"love"'

# Use a sort filter, sort by newest
sort = "newest"

# Select the following fields to return:
# headline, web_url, snippet, source, keywords, pub_date, byline, word_count
field_list = "headline,web_url,snippet,source,keywords,pub_date,byline,word_count"

# Search for reviews published between a begin and end date
begin_date = "20130101"
end_date = "20230531"

# Build URL
query_url = (
    f"{url}api-key={nyt_api_key}&begin_date={begin_date}&end_date={end_date}"
    + f'&fq={filter_query}&sort={sort}&fl={field_list}'
)


# Create an empty list to store the reviews
reviews_list = []

# loop through pages 0-19
for page in range(20):
    # create query with a page number
    # API results show 10 articles at a time
    query_url = (
        f"{url}api-key={nyt_api_key}&begin_date={begin_date}&end_date={end_date}"
        + f"&fq={filter_query}&sort={sort}&fl={field_list}&page={page}"
    )

    
    # Make a "GET" request and retrieve the JSON
    response = requests.get(query_url)
    
    # Add a twelve second interval between queries to stay within API query limits
    time.sleep(12)
    
    # Try and save the reviews to the reviews_list
    try:
        response_json = response.json()
        
        reviews = response_json.get('response', {}).get('docs', [])

        # If no results, print the page number and break from the loop
        if not reviews:
            print(f"No results on page {page}. Breaking the loop.")
            break
            
        # loop through the reviews["response"]["docs"] and append each review to the list
        for review in reviews:
            reviews_list.append({
                "headline": review["headline"]["main"],
                "web_url": review["web_url"],
                "snippet": review["snippet"],
                "source": review["source"],
                "keywords": review["keywords"],
                "pub_date": review["pub_date"],
                "byline": review["byline"],
                "word_count": review["word_count"]
            })

        # Print the page that was just retrieved
        print(f"Checked page {page}")
    # Print the page number that had no results then break from the loop
    except Exception as e:
        print(f"Error on page {page}: {e}")
        break



# Preview the first 5 results in JSON format
# Use json.dumps with argument indent=4 to format data
print(json.dumps(reviews_list[:5], indent=4))


# Convert reviews_list to a Pandas DataFrame using json_normalize()
df_reviews = json_normalize(reviews_list)
df_reviews



# Extract the title from the "headline.main" column and
# save it to a new column "title"
# Title is between unicode characters \u2018 and \u2019. 
# End string should include " Review" to avoid cutting title early

df_reviews['title'] = df_reviews['headline'].apply(
    lambda x: x[x.find('\u2018') + 1:x.rfind('\u2019')] if '\u2018' in x and '\u2019' in x else None
)

df_reviews



# Extract 'name' and 'value' from items in "keywords" column
def extract_keywords(keyword_list):
    extracted_keywords = ""
    for item in keyword_list:
        # Extract 'name' and 'value'
        keyword = f"{item['name']}: {item['value']};" 
        # Append the keyword item to the extracted_keywords list
        extracted_keywords += keyword
    return extracted_keywords

# Fix the "keywords" column by converting cells from a list to a string
df_reviews['keywords'] = df_reviews['keywords'].apply(lambda x: extract_keywords(x) if isinstance(x, list) else x)
df_reviews


# Create a list from the "title" column using to_list()
titles_list = df_reviews['title'].dropna().to_list()

# Display the titles list
titles_list


# Create a list from the "title" column using to_list()
# These titles will be used in the query for The Movie Database
titles_list = df_reviews['title'].dropna().to_list()
titles_list





# Prepare The Movie Database query
url = "https://api.themoviedb.org/3/search/movie?query="
tmdb_key_string = "&api_key=" + tmdb_api_key


# Create an empty list to store the results
tmdb_movies_list = []

# Create a request counter to sleep the requests after a multiple of 50 requests
request_counter = 0

# Prepare The Movie Database query
tmdb_search_url = "https://api.themoviedb.org/3/search/movie?query="
tmdb_key_string = "&api_key=" + tmdb_api_key

# Loop through the titles
for title in titles_list:
    # Check if we need to sleep before making a request
    if request_counter > 0 and request_counter % 50 == 0:
        print("Sleeping for 10 seconds to avoid hitting rate limits.")
        time.sleep(10)

    # Add 1 to the request counter
    request_counter += 1
    
    # Perform a "GET" request for The Movie Database
    search_url = f"{tmdb_search_url}{title}{tmdb_key_string}"
    search_response = requests.get(search_url)
    
    # Include a try clause to search for the full movie details
    try:
        search_data = search_response.json()
        if search_data['results']:
            # Get movie id
            movie_id = search_data['results'][0]['id']

            # Make a request for the full movie details
            details_url = f"https://api.themoviedb.org/3/movie/{movie_id}?api_key={tmdb_api_key}"
            details_response = requests.get(details_url)
            details_data = details_response.json()
            
            # Extract the genre names into a list
            genres = [genre['name'] for genre in details_data.get('genres', [])]
            
            # Extract the spoken_languages' English name into a list
            languages = [language['english_name'] for language in details_data.get('spoken_languages', [])]
            
            # Extract the production_countries' name into a list
            countries = [country['name'] for country in details_data.get('production_countries', [])]
            
            # Add the relevant data to a dictionary and append it to the tmdb_movies_list list
            tmdb_movies_list.append({
                'title': title,
                'id': movie_id,
                'genres': genres,
                'languages': languages,
                'countries': countries,
                'release_date': details_data.get('release_date', ''),
                'runtime': details_data.get('runtime', 0),
                'overview': details_data.get('overview', '')
            })
            
            # Print out the title that was found
            print(f"Found details for title: {title}")
        else:
            print(f"No results found for title: {title}")
    except Exception as e:
        print(f"Error fetching details for title '{title}': {e}")



# Preview the first 5 results in JSON format
# Use json.dumps with argument indent=4 to format data
print(json.dumps(tmdb_movies_list[:5], indent=4))


# Convert the TMDB movies list to a DataFrame
df_tmdb_movies = pd.DataFrame(tmdb_movies_list)
df_tmdb_movies





# Merge the New York Times reviews and TMDB DataFrames on title
merged_df = pd.merge(df_reviews, df_tmdb_movies, on='title', how='inner')
merged_df


# Remove list brackets and quotation marks on the columns containing lists
# Create a list of the columns that need fixing
columns_to_fix = ['genres', 'languages', 'countries', 'byline.person']

# Create a list of characters to remove
characters_to_remove = ['[', ']', "'", '{', '}', ':']

def remove_characters(string, characters):
    for char in characters:
        string = string.replace(char, '')
    return string
# Loop through the list of columns to fix
for column in columns_to_fix:
    # Convert the column to type 'str'
    merged_df[column] = merged_df[column].astype(str)
    # Loop through characters to remove
    merged_df[column] = merged_df[column].apply(lambda x: remove_characters(x, characters_to_remove))


# Display the fixed DataFrame
merged_df




# Drop "byline.person" column
merged_drop_df = merged_df.drop(columns=['byline.person'])
merged_drop_df


# Delete duplicate rows and reset index
merged_dd_df = merged_df.drop_duplicates()

# Reset the index
final_list_df = merged_dd_df.reset_index(drop=True)
final_list_df


# Export data to CSV without the index
final_list_df.to_csv('final_list.csv', index=False)




